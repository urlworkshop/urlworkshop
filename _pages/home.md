---
layout: project
urltitle:  "Unsupervised Reinforcement Learning (URL)"
title: "Unsupervised Reinforcement Learning (URL)"
categories: workshop, icml, unsupervised learning, self-supervised learning, reinforcement learning, deep learning, 2021
permalink: /
bibtex: true
paper: true
acknowledgements: ""
---
<div class="alert" role="alert">



</div>
<div class="row reverse">
  <div class="col-xs-12 col-md-7">
    <h1>Unsupervised Reinforcement Learning Workshop</h1>
    <br>
    <h4>July , 2021 // ICML Workshop</h4>
    <br>
    <br>
    <p>
        Unsupervised learning has begun to deliver on its promise in the recent past with tremendous progress made in the fields of natural language processing and computer vision whereby large scale unsupervised pre-training has enabled fine-tuning to downstream supervised learning tasks with limited labeled data. This is particularly encouraging and appealing in the context of reinforcement learning considering that it is expensive to perform rollouts in the real world with annotations either in the form of reward signals or human demonstrations. We therefore believe that a workshop in the intersection of unsupervised and reinforcement learning is timely and we hope to bring together researchers with diverse views on how to make further progress in this exciting and open-ended subfield.

    </p>
  </div>
</div>

</div>
<div class="row reverse">
    <img class="cover" src="/static/img/cover.jpeg">
</div>


<br />

<div class="row" id="dates">
  <div class="col-xs-12">
    <h2>Important Dates</h2>
  </div>
</div>

<br>
<div class="row">
  <div class="col-xs-12">
    <table class="table table-striped">
      <tbody>
        <tr>
          <td>Paper Submission Deadline</td>
          <td><s>June 9, 2021 AoE</td>
        </tr>
        <tr>
          <td>Decision Notifications</td>
          <td>June 30, 2021 </td>
        </tr>
        <tr>
          <td>Camera Ready Paper Deadline</td>
          <td>July 17, 2021 AoE</td>
        </tr>
        <tr>
          <td>Workshop</td>
          <td>July 24, 2021</td>
        </tr>
      </tbody>
    </table>
  </div>
</div>

<br />

<div class="row" id="cfp">
  <div class="col-xs-12">
    <h2>Call for Papers</h2>
  </div>
</div>






<div class="row">
  <div class="col-xs-12">
    <p>
      We invite both short (4 page) and long (8 page) anonymized submissions in the <a href="https://media.icml.cc/Conferences/ICML2021/Styles/icml2021_style.zip">ICML LaTeX format</a> that develop algorithms, benchmarks, and ideas to allow reinforcement learning agents to learn more effectively by making self-supervised predictions about their environment. More concretely, we welcome submissions around, but not necessarily limited to, the following broad questions:
    </p>
    <p>
          <ul>
              <li>How can we leverage large amounts of unlabelled sensory data from diverse sources to bootstrap learning for reinforcement learning tasks?</li>
              <li>Can we use auxiliary targets --- generated in an unsupervised manner --- to accelerate learning?</li>
              <li>Can we meta-learn auxiliary targets that accelerate learning for a wide range of tasks?</li>
              <li>Can we build benchmarks and protocols for systematically comparing existing self-supervision methods? </li>
              <li>Do agents that learn multiple auxiliary predictions generalize better to new environments than those that learn purely by maximizing the reward?</li>
              <li>Can we learn predictive state representations to accelerate learning and generalization?  </li>
              <li>Can we benefit from insights gained from cognitive and neuroscience to build better self-supervisory objectives? </li>
          </ul>
      </p>
      <p>We welcome review and positional papers that may foster discussions. We also encourage published papers from <i>*non-ML*</i> conferences, e.g. epistemology, cognitive science, psychology, neuroscience, that are within the scope of the workshop.
       Note that as per ICLR guidelines, we don't accept works previously published in other conferences on machine learning, but are open to works that are currently under submission to a conference (such as ICML 2021).</p>

      <p>
        Submissions should be uploaded on OpenReview: <a class="red" href="https://openreview.net/group?id=ICML.cc/2021/Workshop/URL">URL submission link</a>
      </p>
      <p>
        In case of any issues or questions, feel free to email the workshop organizers at: <a href="mailto:url.icml2021@gmail.com" class="red">url.icml2021@gmail.com</a>.
      </p>

  </div>
</div>


<br />


<hr />

<div class="row" id="speakers">
  <div class="col-xs-12">
    <h2>Speakers</h2>
  </div>
</div>

<div class="row">
  <div class="col-xs-6 col-lg-3 people">
    <a href="https://ai.stanford.edu/~cbfinn/">
      <img class="people-pic" src="{{ "/static/img/people/chelsea_finn.jpeg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://ai.stanford.edu/~cbfinn/">Chelsea Finn</a>
      <h6>Stanford</h6>
    </div>
  </div>
  <div class="col-xs-6 col-lg-3 people">
    <a href="https://web.mit.edu/krallen/www/">
      <img class="people-pic" src="{{ "/static/img/people/kelsey_allen.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://web.mit.edu/krallen/www/">Kelsey Allen</a>
      <h6>DeepMind</h6>
    </div>
  </div>
  <div class="col-xs-6 col-lg-3 people">
    <a href="https://danijar.com/">
      <img class="people-pic" src="{{ "/static/img/people/danijar_hafner.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://danijar.com/">Danijar Hafner</a>
      <h6>University of Toronto</h6>
    </div>
  </div>
  <div class="col-xs-6 col-lg-3 people">
    <a href="https://nke001.github.io/">
      <img class="people-pic" src="{{ "/static/img/people/nan_rosemary_ke.jpeg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://nke001.github.io/">Nan Rosemary Ke</a>
      <h6>MILA</h6>
    </div>
  </div>
  <div class="col-xs-6 col-lg-3 people">
    <a href="http://chercheurs.lille.inria.fr/~lazaric/Webpage/Home/Home.html">
      <img class="people-pic" src="{{ "/static/img/people/alessandro_lazaric.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="http://chercheurs.lille.inria.fr/~lazaric/Webpage/Home/Home.html">Alessandro Lazaric</a>
      <h6>Facebook AI Research</h6>
    </div>
  </div>
  <div class="col-xs-6 col-lg-3 people">
    <a href="http://www.cs.umd.edu/~kdbrant/">
      <img class="people-pic" src="{{ "/static/img/people/kiante_brantley.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="http://www.cs.umd.edu/~kdbrant/">Kiante Brantley</a>
      <h6>Maryland College Park</h6>
    </div>
  </div>
  <div class="col-xs-6 col-lg-3 people">
    <a href="https://research.google/people/105004/">
      <img class="people-pic" src="{{ "/static/img/people/david_ha.jpeg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://research.google/people/105004/">David Ha</a>
      <h6>Google Brain</h6>
    </div>
  </div>
</div>

<hr />

<div class="row" id="intro">
    <div class="col-xs-12">
        <h2>Introduction</h2>
        For decades unsupervised learning (UL) has promised to drastically reduce our reliance on supervision and reinforcement. Now, in the last couple of years, unsupervised learning has been delivering on this problem with substantial advances in computer vision [e.g., CPC (Oord et al., 2018), SimCLR (Chen et al., 2020), MoCo (He et al., 2019), BYOL (Grill et al., 2020)] and natural language processing [e.g., BERT (Devlin et al., 2018), GPT-x (OpenAI), T5 (Raffel et al., 2019), Roberta (Liu et al., 2019)]. The general purpose representations learned by unsupervised methods are useful for a variety of downstream supervised learning tasks, particularly in the low data regime [BERT (Devlin et al., 2018), GPT-3 (OpenAI), T5 (Rafel et al., 2019), CPCv2 (Henaff et al., 2019), SimCLR (Chen et al., 2020), SimCLRv2 (Chen et el., 2020)].

However, in the context of reinforcement learning, we havenâ€™t seen the level of impact UL has had in vision and language. This is not for the lack of trying. There has been a wide variety of methods developed by the Machine Learning community to use UL to make a meaningful impact in RL. A few prominent directions are as follows:

Learning rich representations of high dimensional observations to aid reinforcement learning [UNREAL (Jaderberg et el., 2016), DARLA (Higgins et al., 2017), TCN (Sermanet et al., 2017), SAC-AE (Yarats et al., 2019), SLAC (Lee et al., 2019), CURL (Srinivas et al., 2020), ATC (Stooke et al., 2020), Bisimulation (Zhang et al., 2020)]
Building world models for planning [Visual MPC (Hirose et al., 2019), World Models (Hafner et al., 2020), Simple (Kaiser et al., 2019), PlaNet (Hafner et al., 2018), Dreamer (Hafner et al., 2019), MuZero (Schrittwieser et al., 2019)]
Learning to explore environments with sparse reward signals [EX2 (Fu et al., 2019), Curiosity (Pathak et al., 2019), RND (Burda et al., 2018)]
Learning task agnostic, diverse and reusable skills [VIC (Gregor et al., 2016), VALOR (Achiam et al., 2018), DIAYN (Eysenbach et al., 2018),  DISCERN (Warde-Farley et al., 2018), DADS (Sharma et al., 2019)]
Extracting signals for free with goal-conditioned and hindsight models [UVFA (Schaul et al., 2015), HER (Andrychowicz et al., 2017), Asymmetric Self-Play (Sukhbaatar et al., 2017), RIG (Nair et al., 2017), DPN (Yu et al., 2017), Learning From Play (Lynch  et al., 2019)]
Unsupervised Learning in the context of Meta/Multi-Task Learning [CARML (Jabri et al., 2019), UML (Gupta et al., 2018)]

These early works suggest that unsupervised approaches could significantly improve RL algorithms. However, this research direction is still in its infancy, which leaves many open questions regarding the best ways of combining unsupervised learning with RL. As a community we still do not know how to evaluate learned representations, measure the impact of UL, and understand how UL can benefit RL the most. The workshop discussions and presentations aim to address, among others, the following questions and topics:

How can the use of UL advance RL?
What are the most effective ways of combining UL with RL?
What are the settings in which UL can be most beneficial in RL?
How is Representation Learning for RL different than for downstream supervised tasks?
How can UL improve RL in terms of sample efficiency, generalization, exploration?
How can UL and Skill Discovery be maximally synergetic?
How does the role of UL differ across Model-based RL, Model-free On-policy RL, Model-free Off-policy RL, Offline RL?
What inspirations can we take from cognitive science to bridge to inspire the next crop of UL methods for RL?
Is there a unified view to combine different UL methods into a single framework?

This workshop will bring together researchers working in unsupervised learning (including those in computer vision or natural language processing), representation learning and reinforcement learning to discuss the benefits, challenges and potential solutions for effectively using unsupervised learning techniques to enhance reinforcement learning agents. Early workshops were crucial to accelerate the use of UL techniques in vision and language, and we hope this workshop will serve as the kindling for UL techniques in RL.


        <p> The aims of this workshop are to explore the potential benefits of self-supervision, how to specify self-supervised tasks, and to bring together people from different areas, including Cognitive Science, Reinforcement Learning, and Computer Vision, with a common interest in building better learning agents. The specific research questions we hope to tackle include:</p>
        <ul>
        <li>How can we leverage unsupervised data to bootstrap learning in an MDP, and what primitives should we learn: dynamics, representation, skills or something else?</li>
        <li>How can we measure progress in development of self-supervised, general purpose agents? Do we need to create a GLUE like benchmark for RL?</li>
        <li>What kind of structure in an MDP can an agent exploit to learn a task faster?</li>
        <li>How can we design self-supervised objectives that encourage an agent to generalize well out of its training distribution?</li>
        <li>Can we leverage insights from cognitive science on how humans acquire knowledge to build better self-supervised objectives?</li>
        </ul>
    </div>
</div>

<hr />


<hr />

<div class="row" id="organizers">
  <div class="col-xs-12">
    <h2>Organizers</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-6 col-lg-3 people">
    <a href="https://feryal.github.io/">
      <img class="people-pic" src="{{ "/static/img/people/feryal_behbahani.jpeg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://feryal.github.io/">Feryal Behbahani</a>
      <h6>DeepMind</h6>
    </div>
  </div>
  <div class="col-xs-6 col-lg-3 people">
    <a href="https://www.cs.mcgill.ca/~jpineau/">
      <img class="people-pic" src="{{ "/static/img/people/joelle_pineau.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://www.cs.mcgill.ca/~jpineau/">Joelle Pineau</a>
      <h6>McGill University / Mila / FAIR</h6>
    </div>
  </div>
  <div class="col-xs-6 col-lg-3 people">
    <a href="https://www.lerrelpinto.com/">
      <img class="people-pic" src="{{ "/static/img/people/lerrel_pinto.jpeg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://www.lerrelpinto.com/">Lerrel Pinto</a>
      <h6>NYU</h6>
    </div>
  </div>
  <div class="col-xs-6 col-lg-3 people">
    <a href="https://rraileanu.github.io/">
      <img class="people-pic" src="{{ "/static/img/people/roberta_raileanu.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name people">
      <a href="https://rraileanu.github.io/">Roberta Raileanu</a>
      <h6>NYU</h6>
    </div>
  </div>
  <div class="col-xs-6 col-lg-3 people">
    <a href="https://amyzhang.github.io/">
      <img class="people-pic" src="{{ "/static/img/people/amy_zhang.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://amyzhang.github.io/">Amy Zhang</a>
      <h6>McGill University / Mila / FAIR</h6>
    </div>
  </div>
  <div class="col-xs-6 col-lg-3 people">
    <a href="https://people.eecs.berkeley.edu/~aravind/">
      <img class="people-pic" src="{{ "/static/img/people/aravind_srinivas.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://people.eecs.berkeley.edu/~aravind/">Aravind Srinivas</a>
      <h6>UC Berkeley</h6>
    </div>
  </div>
  <div class="col-xs-6 col-lg-3 people">
    <a href="http://denis-yarats.info/">
      <img class="people-pic" src="{{ "/static/img/people/denis_yarats.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="http://denis-yarats.info/">Denis Yarats</a>
      <h6>NYU / FAIR</h6>
    </div>
  </div>
</div>

<hr />


<div class="row">
  <div class="col-xs-12">
    <h2>References</h2>
  </div>
</div>
<div class="row">
  <div class="col-md-12">
    <ol>
<li>Finn, Chelsea, Ian Goodfellow, and Sergey Levine. "Unsupervised learning for physical interaction through video prediction." NeurIPS (2016).</li>
<li>Ha, David, and JÃ¼rgen Schmidhuber. "Recurrent world models facilitate policy evolution." NeurIPS (2018). </li>
<li>Hafner, Danijar, et al. "Learning latent dynamics for planning from pixels." ICML (2019).  </li>
<li>Kipf, Thomas, Elise van der Pol, and Max Welling. "Contrastive learning of structured world models." arXiv.  (2019). </li>
<li>Schmidhuber, JÃ¼rgen. "A possibility for implementing curiosity and boredom in model-building neural controllers." Proc. of the international conference on simulation of adaptive behavior: From animals to animals.  (1991). </li>
<li>Klyubin, Alexander S., Daniel Polani, and Chrystopher L. Nehaniv. "Empowerment: A universal agent-centric measure of control." IEEE Congress on Evolutionary Computation.  (2005). </li>
<li>Sutton, Richard S., et al. "Horde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction." The 10th International Conference on Autonomous Agents and Multiagent Systems-Volume 2. 2011. </li>
<li>Mohamed, Shakir, and Danilo Jimenez Rezende. "Variational information maximisation for intrinsically motivated reinforcement learning." NeurIPS (2015). </li>
<li>Pathak, Deepak, et al. "Curiosity-driven exploration by self-supervised prediction." Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops.  (2017). </li>
<li>Ebert, Frederik, et al. "Visual foresight: Model-based deep reinforcement learning for vision-based robotic control." arXiv.  (2018). </li>
<li>Sekar, Ramanan, et al. "Planning to Explore via Self-Supervised World Models." arXiv.  (2020). </li>
<li>Lynch, Corey, et al. "Learning latent plans from play." CoRL. (2020). </li>
<li>Jaderberg, Max, et al. "Reinforcement learning with unsupervised auxiliary tasks." arXiv.(2016). </li>
<li>Eslami, SM Ali, et al. "Neural scene representation and rendering." Science. (2018). </li>
<li>Anand, Ankesh, et al. "Unsupervised state representation learning in atari." NeurIPS (2019). </li>
<li>Srinivas, Aravind et al. "CURL: Contrastive Unsupervised Representations for Reinforcement Learning" ICML (2020). </li>
<li>Zhang, Amy, et al. "Learning invariant representations for reinforcement learning without reconstruction." arXiv (2020).  </li>
<li>Mazoure, Bogdan, et al. "Deep reinforcement and infomax learning." NeurIPS (2020). </li>
<li> Stooke, Adam, et al. "Decoupling representation learning from reinforcement learning." arXiv preprint arXiv:2009.08319 (2020). </li>
<li>Hansen, Nicklas, et al. "Self-Supervised Policy Adaptation during Deployment." arXiv (2020). </li>
<li> Agarwal, Rishab, et al. "Contrastive Behavioral Similarity Embeddings for Generalization in Reinforcement Learning" ICLR (2021). </li>
<li>Schwarzer, Max, et al. "Data-Efficient Reinforcement Learning with Self-Predictive Representations." ICLR (2021) </li>

</ol>
  </div>
</div>

<div class="text-center p-3" style="background-color: rgba(0, 0, 0, 0)">
    <h6>Website theme inspired from the <a href="https://github.com/vigilworkshop/vigilworkshop.github.io">VIGIL workshop</a>. Cover art by <a href="https://www.mattdixon.co.uk">Matt Dixon</a></h6>
  </div>
